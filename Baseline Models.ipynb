{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "Basic baseline models for next utterance classification using:\n",
    "1. completely random predictor\n",
    "2. TF-IDF\n",
    "3. word2vec (with averaging)\n",
    "4. doc2vec\n",
    "5. LDA\n",
    "\n",
    "methods for vectorizing messages. Evaluation using recall@k on test set with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from gensim.models import doc2vec, word2vec\n",
    "import gensim\n",
    "from collections import namedtuple\n",
    "import timeit\n",
    "import csv\n",
    "\n",
    "from modules.evaluation_metrics import recall_at_k\n",
    "from modules import tfidf\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "valid = pd.read_csv(\"data/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data into right format i.e. list of strings\n",
    "train_data = np.append(train.Context.values,train.Utterance.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i think we could import the old comment via rs...</td>\n",
       "      <td>basic each xfree86 upload will not forc user t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i 'm not suggest all - onli the one you modifi...</td>\n",
       "      <td>sorri __eou__ i think it be ubuntu relat . __e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afternoon all __eou__ not entir relat to warti...</td>\n",
       "      <td>yep . __eou__ oh , okay . i wonder what happen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interest __eou__ grub-instal work with / be ex...</td>\n",
       "      <td>that the one __eou__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and becaus python give mark a woodi __eou__ __...</td>\n",
       "      <td>( i think someon be go to make a joke about .a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  i think we could import the old comment via rs...   \n",
       "1  i 'm not suggest all - onli the one you modifi...   \n",
       "2  afternoon all __eou__ not entir relat to warti...   \n",
       "3  interest __eou__ grub-instal work with / be ex...   \n",
       "4  and becaus python give mark a woodi __eou__ __...   \n",
       "\n",
       "                                           Utterance  Label  \n",
       "0  basic each xfree86 upload will not forc user t...      1  \n",
       "1  sorri __eou__ i think it be ubuntu relat . __e...      0  \n",
       "2  yep . __eou__ oh , okay . i wonder what happen...      0  \n",
       "3                               that the one __eou__      1  \n",
       "4  ( i think someon be go to make a joke about .a...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train.shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18920, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Ground Truth Utterance</th>\n",
       "      <th>Distractor_0</th>\n",
       "      <th>Distractor_1</th>\n",
       "      <th>Distractor_2</th>\n",
       "      <th>Distractor_3</th>\n",
       "      <th>Distractor_4</th>\n",
       "      <th>Distractor_5</th>\n",
       "      <th>Distractor_6</th>\n",
       "      <th>Distractor_7</th>\n",
       "      <th>Distractor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anyon know whi my stock oneir export env var u...</td>\n",
       "      <td>nice thank ! __eou__</td>\n",
       "      <td>wrong channel for it , but check efnet.org , u...</td>\n",
       "      <td>everi time the kernel chang , you will lose vi...</td>\n",
       "      <td>ok __eou__</td>\n",
       "      <td>! nomodeset &gt; acer __eou__ i 'm assum it be a ...</td>\n",
       "      <td>http : //www.ubuntu.com/project/about-ubuntu/d...</td>\n",
       "      <td>thx __eou__ unfortun the program be n't instal...</td>\n",
       "      <td>how can i check ? by do a recoveri for test ? ...</td>\n",
       "      <td>my humbl apolog __eou__</td>\n",
       "      <td># ubuntu-offtop __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i set up my hd such that i have to type a pass...</td>\n",
       "      <td>so you dont know , ok , anyon els ? __eou__ yo...</td>\n",
       "      <td>nmap be nice , but it be n't what i be look fo...</td>\n",
       "      <td>ok __eou__</td>\n",
       "      <td>cdrom work fine on window . __eou__ i dont thi...</td>\n",
       "      <td>ah yes , i have read return as rerun __eou__</td>\n",
       "      <td>hm ? __eou__</td>\n",
       "      <td>not the case , lts be everi other .04 releas ....</td>\n",
       "      <td>pretti much __eou__</td>\n",
       "      <td>i use the one i download from amd __eou__</td>\n",
       "      <td>ffmpeg be part of the packag , quixotedon , at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im tri to use ubuntu on my macbook pro retina ...</td>\n",
       "      <td>just wonder how it run __eou__</td>\n",
       "      <td>yes , that 's what i do , export it to a `` id...</td>\n",
       "      <td>noth - i be talk about the question of myhero ...</td>\n",
       "      <td>that should fix the font be too larg __eou__</td>\n",
       "      <td>okay , so hcitool echo back hci0 &lt; mac address...</td>\n",
       "      <td>i get to the menu with option such as tri ubun...</td>\n",
       "      <td>whi do u need analyz __eou__ it be a toy __eou...</td>\n",
       "      <td>cntrl-c may stop the command but it doe n't fi...</td>\n",
       "      <td>if you re onli go to run ubuntu , just get a n...</td>\n",
       "      <td>the one which be not pick up at the moment be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no suggest ? __eou__ link ? __eou__ how can i ...</td>\n",
       "      <td>you cant load anyth via usb or cd when luk be ...</td>\n",
       "      <td>-p sorri ... __eou__ nmap -p22 __eou__ it doe ...</td>\n",
       "      <td>i guess so i ca n't even launch it . __eou__</td>\n",
       "      <td>note __eou__</td>\n",
       "      <td>rxvt-unicod be one __eou__</td>\n",
       "      <td>i tar all of ~ __eou__</td>\n",
       "      <td>i tar all of ~ __eou__</td>\n",
       "      <td>i do n't realli know if i can help , but i be ...</td>\n",
       "      <td>that work just fine , thank ! __eou__</td>\n",
       "      <td>thank you __eou__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just ad a second usb printer but not sure wh...</td>\n",
       "      <td>i be set it up under the printer configur __eo...</td>\n",
       "      <td>i 'd say the most common venu would be via lau...</td>\n",
       "      <td>the old hardi man page , http : //manpages.ubu...</td>\n",
       "      <td>i ll give a tri __eou__</td>\n",
       "      <td>by the way , the url you post for davf be from...</td>\n",
       "      <td>http : //ubuntuforums.org/showthread.php ? t=1...</td>\n",
       "      <td>so i load up putti gui , then what do i do ? _...</td>\n",
       "      <td>you should read error messag , it say be you r...</td>\n",
       "      <td>wait the colleg semest to close just to make s...</td>\n",
       "      <td>i be call myself a jerk . all i know be that y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  anyon know whi my stock oneir export env var u...   \n",
       "1  i set up my hd such that i have to type a pass...   \n",
       "2  im tri to use ubuntu on my macbook pro retina ...   \n",
       "3  no suggest ? __eou__ link ? __eou__ how can i ...   \n",
       "4  i just ad a second usb printer but not sure wh...   \n",
       "\n",
       "                              Ground Truth Utterance  \\\n",
       "0                               nice thank ! __eou__   \n",
       "1  so you dont know , ok , anyon els ? __eou__ yo...   \n",
       "2                     just wonder how it run __eou__   \n",
       "3  you cant load anyth via usb or cd when luk be ...   \n",
       "4  i be set it up under the printer configur __eo...   \n",
       "\n",
       "                                        Distractor_0  \\\n",
       "0  wrong channel for it , but check efnet.org , u...   \n",
       "1  nmap be nice , but it be n't what i be look fo...   \n",
       "2  yes , that 's what i do , export it to a `` id...   \n",
       "3  -p sorri ... __eou__ nmap -p22 __eou__ it doe ...   \n",
       "4  i 'd say the most common venu would be via lau...   \n",
       "\n",
       "                                        Distractor_1  \\\n",
       "0  everi time the kernel chang , you will lose vi...   \n",
       "1                                         ok __eou__   \n",
       "2  noth - i be talk about the question of myhero ...   \n",
       "3       i guess so i ca n't even launch it . __eou__   \n",
       "4  the old hardi man page , http : //manpages.ubu...   \n",
       "\n",
       "                                        Distractor_2  \\\n",
       "0                                         ok __eou__   \n",
       "1  cdrom work fine on window . __eou__ i dont thi...   \n",
       "2       that should fix the font be too larg __eou__   \n",
       "3                                       note __eou__   \n",
       "4                            i ll give a tri __eou__   \n",
       "\n",
       "                                        Distractor_3  \\\n",
       "0  ! nomodeset > acer __eou__ i 'm assum it be a ...   \n",
       "1       ah yes , i have read return as rerun __eou__   \n",
       "2  okay , so hcitool echo back hci0 < mac address...   \n",
       "3                         rxvt-unicod be one __eou__   \n",
       "4  by the way , the url you post for davf be from...   \n",
       "\n",
       "                                        Distractor_4  \\\n",
       "0  http : //www.ubuntu.com/project/about-ubuntu/d...   \n",
       "1                                       hm ? __eou__   \n",
       "2  i get to the menu with option such as tri ubun...   \n",
       "3                             i tar all of ~ __eou__   \n",
       "4  http : //ubuntuforums.org/showthread.php ? t=1...   \n",
       "\n",
       "                                        Distractor_5  \\\n",
       "0  thx __eou__ unfortun the program be n't instal...   \n",
       "1  not the case , lts be everi other .04 releas ....   \n",
       "2  whi do u need analyz __eou__ it be a toy __eou...   \n",
       "3                             i tar all of ~ __eou__   \n",
       "4  so i load up putti gui , then what do i do ? _...   \n",
       "\n",
       "                                        Distractor_6  \\\n",
       "0  how can i check ? by do a recoveri for test ? ...   \n",
       "1                                pretti much __eou__   \n",
       "2  cntrl-c may stop the command but it doe n't fi...   \n",
       "3  i do n't realli know if i can help , but i be ...   \n",
       "4  you should read error messag , it say be you r...   \n",
       "\n",
       "                                        Distractor_7  \\\n",
       "0                            my humbl apolog __eou__   \n",
       "1          i use the one i download from amd __eou__   \n",
       "2  if you re onli go to run ubuntu , just get a n...   \n",
       "3              that work just fine , thank ! __eou__   \n",
       "4  wait the colleg semest to close just to make s...   \n",
       "\n",
       "                                        Distractor_8  \n",
       "0                            # ubuntu-offtop __eou__  \n",
       "1  ffmpeg be part of the packag , quixotedon , at...  \n",
       "2  the one which be not pick up at the moment be ...  \n",
       "3                                  thank you __eou__  \n",
       "4  i be call myself a jerk . all i know be that y...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print test.shape\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19560, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Ground Truth Utterance</th>\n",
       "      <th>Distractor_0</th>\n",
       "      <th>Distractor_1</th>\n",
       "      <th>Distractor_2</th>\n",
       "      <th>Distractor_3</th>\n",
       "      <th>Distractor_4</th>\n",
       "      <th>Distractor_5</th>\n",
       "      <th>Distractor_6</th>\n",
       "      <th>Distractor_7</th>\n",
       "      <th>Distractor_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ani idea on how lts will be releas ? __eou__ _...</td>\n",
       "      <td>we be talk 12.04 not 10.04 __eou__</td>\n",
       "      <td>you rememb my flash issu from yesterday or the...</td>\n",
       "      <td>oh , no idea other be probabl ok __eou__ updat...</td>\n",
       "      <td>no , greenit be say his download speed be slow...</td>\n",
       "      <td>lsb_releas -sc __eou__ well ... regardless . i...</td>\n",
       "      <td>you can buy _anything_ in china __eou__</td>\n",
       "      <td>no __eou__</td>\n",
       "      <td>sudo restart lightdm __eou__</td>\n",
       "      <td>you be still ask for the uniti logout menu rig...</td>\n",
       "      <td>so i be work as a linux admin intern , and my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how much hdd use ubuntu default instal ? __eou...</td>\n",
       "      <td>that whi i ask how much be default instal ? : ...</td>\n",
       "      <td>all of this possibl in older version of ubuntu...</td>\n",
       "      <td>: be that a question ? __eou__</td>\n",
       "      <td>yes __eou__</td>\n",
       "      <td>thank __eou__ i would imagin so , the site bon...</td>\n",
       "      <td>yes i ve investig that alreadi . it seem you c...</td>\n",
       "      <td>not realli . i use urxvt myself . __eou__</td>\n",
       "      <td>thank a lot , realli ! __eou__</td>\n",
       "      <td>as someon els suggest , close update-manag , a...</td>\n",
       "      <td>you re welcom .. sinc 12.04 throw dnsmasq into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in my countri it near the 27th __eou__ when wi...</td>\n",
       "      <td>thanx __eou__</td>\n",
       "      <td>i have no .docx file , so do n't know , whi no...</td>\n",
       "      <td>i ve boot countless distro from usb on my aao ...</td>\n",
       "      <td>but i 'm sure i can work it out __eou__</td>\n",
       "      <td>the way you put it , that sound like a sever c...</td>\n",
       "      <td>im not familiar with hotspot __eou__</td>\n",
       "      <td>it work fine without set up an ssh tunnel manu...</td>\n",
       "      <td>so it have two be a two-command process ? __eou__</td>\n",
       "      <td>and becaus you onli have 3 gb of ram , be not ...</td>\n",
       "      <td>it ok but no error ? then how do you know it a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it 's not out __eou__ __eot__ they probabali b...</td>\n",
       "      <td>wait for mani thing to be setup __eou__ final ...</td>\n",
       "      <td>that 's right , while chat i regrett make a lo...</td>\n",
       "      <td>afaik it 's best to start at 2mb = 2048k __eou__</td>\n",
       "      <td>for the most part , you should be instal pytho...</td>\n",
       "      <td>do you overwrit your win instal or can you bro...</td>\n",
       "      <td>for some reason the headphon option doe not ch...</td>\n",
       "      <td>well then i do n't know . can anyth boot on th...</td>\n",
       "      <td>well then i do n't know . can anyth boot on th...</td>\n",
       "      <td>ya , but i guess you could do a git of your en...</td>\n",
       "      <td>noexec be a mount option . you would have to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be the ext4 driver stabl ? __eou__ __eot__ i b...</td>\n",
       "      <td>you sound like it 's updat to skynet . ; ) __e...</td>\n",
       "      <td>ok i will tri that , brb __eou__ it complain a...</td>\n",
       "      <td>ouch __eou__</td>\n",
       "      <td>i do system annalysi and it say everyth pass 1...</td>\n",
       "      <td>not to mention way less complex ... you can ha...</td>\n",
       "      <td>well , you can , accord to that articl , i als...</td>\n",
       "      <td>if not , i think you can pretti much grab ani ...</td>\n",
       "      <td>gpart ? i do n't want do edit partit , just mo...</td>\n",
       "      <td>i ve tri it . not a fan at all __eou__ i have ...</td>\n",
       "      <td>ah , okay __eou__</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  ani idea on how lts will be releas ? __eou__ _...   \n",
       "1  how much hdd use ubuntu default instal ? __eou...   \n",
       "2  in my countri it near the 27th __eou__ when wi...   \n",
       "3  it 's not out __eou__ __eot__ they probabali b...   \n",
       "4  be the ext4 driver stabl ? __eou__ __eot__ i b...   \n",
       "\n",
       "                              Ground Truth Utterance  \\\n",
       "0                 we be talk 12.04 not 10.04 __eou__   \n",
       "1  that whi i ask how much be default instal ? : ...   \n",
       "2                                      thanx __eou__   \n",
       "3  wait for mani thing to be setup __eou__ final ...   \n",
       "4  you sound like it 's updat to skynet . ; ) __e...   \n",
       "\n",
       "                                        Distractor_0  \\\n",
       "0  you rememb my flash issu from yesterday or the...   \n",
       "1  all of this possibl in older version of ubuntu...   \n",
       "2  i have no .docx file , so do n't know , whi no...   \n",
       "3  that 's right , while chat i regrett make a lo...   \n",
       "4  ok i will tri that , brb __eou__ it complain a...   \n",
       "\n",
       "                                        Distractor_1  \\\n",
       "0  oh , no idea other be probabl ok __eou__ updat...   \n",
       "1                     : be that a question ? __eou__   \n",
       "2  i ve boot countless distro from usb on my aao ...   \n",
       "3   afaik it 's best to start at 2mb = 2048k __eou__   \n",
       "4                                       ouch __eou__   \n",
       "\n",
       "                                        Distractor_2  \\\n",
       "0  no , greenit be say his download speed be slow...   \n",
       "1                                        yes __eou__   \n",
       "2            but i 'm sure i can work it out __eou__   \n",
       "3  for the most part , you should be instal pytho...   \n",
       "4  i do system annalysi and it say everyth pass 1...   \n",
       "\n",
       "                                        Distractor_3  \\\n",
       "0  lsb_releas -sc __eou__ well ... regardless . i...   \n",
       "1  thank __eou__ i would imagin so , the site bon...   \n",
       "2  the way you put it , that sound like a sever c...   \n",
       "3  do you overwrit your win instal or can you bro...   \n",
       "4  not to mention way less complex ... you can ha...   \n",
       "\n",
       "                                        Distractor_4  \\\n",
       "0            you can buy _anything_ in china __eou__   \n",
       "1  yes i ve investig that alreadi . it seem you c...   \n",
       "2               im not familiar with hotspot __eou__   \n",
       "3  for some reason the headphon option doe not ch...   \n",
       "4  well , you can , accord to that articl , i als...   \n",
       "\n",
       "                                        Distractor_5  \\\n",
       "0                                         no __eou__   \n",
       "1          not realli . i use urxvt myself . __eou__   \n",
       "2  it work fine without set up an ssh tunnel manu...   \n",
       "3  well then i do n't know . can anyth boot on th...   \n",
       "4  if not , i think you can pretti much grab ani ...   \n",
       "\n",
       "                                        Distractor_6  \\\n",
       "0                       sudo restart lightdm __eou__   \n",
       "1                     thank a lot , realli ! __eou__   \n",
       "2  so it have two be a two-command process ? __eou__   \n",
       "3  well then i do n't know . can anyth boot on th...   \n",
       "4  gpart ? i do n't want do edit partit , just mo...   \n",
       "\n",
       "                                        Distractor_7  \\\n",
       "0  you be still ask for the uniti logout menu rig...   \n",
       "1  as someon els suggest , close update-manag , a...   \n",
       "2  and becaus you onli have 3 gb of ram , be not ...   \n",
       "3  ya , but i guess you could do a git of your en...   \n",
       "4  i ve tri it . not a fan at all __eou__ i have ...   \n",
       "\n",
       "                                        Distractor_8  \n",
       "0  so i be work as a linux admin intern , and my ...  \n",
       "1  you re welcom .. sinc 12.04 throw dnsmasq into...  \n",
       "2  it ok but no error ? then how do you know it a...  \n",
       "3  noexec be a mount option . you would have to c...  \n",
       "4                                  ah , okay __eou__  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print valid.shape\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completely Random Predictor\n",
    "This naive predictor randomly picks one of the responses for each of the rows in the test data set. Therefore, we would expect it to have ~10% accuracy with recall@1, ~20% accuracy with recall@2, ~50% accuracy with recall@5 and 100% accuracy with recall@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Completely random prediction model\n",
    "def random_predictor(context, test):\n",
    "    return np.random.choice(len(test), 10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.0974101\n",
      "Recall @ 2, 10 total choices: 0.195032\n",
      "Recall @ 5, 10 total choices: 0.493869\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# As a sanity check, let's see if the random predictor performs as expected\n",
    "y_random = [random_predictor(test.Context[x], test.iloc[x,1:].values) for x in range(len(test))]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y_random, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random predictor does indeed perform as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighting\n",
    "Now let's try to use **TF-IDF** weighting to vectorize the contexts and responses and use **cosine similarity** to compute the rank each of the possible responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit TF-IDF model\n",
    "tfidf_model = tfidf.TFIDF_Predictor()\n",
    "tfidf_model.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.495032\n",
      "Recall @ 2, 10 total choices: 0.596882\n",
      "Recall @ 5, 10 total choices: 0.766121\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "y = [tfidf_model.predict(test.Context[x], test.iloc[x,1:].values) for x in range(test.shape[0])]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "Next let's generate word embeddings using word2vec and average the word2vec representations for each word in a message in order to get a feature vector for that message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each of the messages into lists of words\n",
    "train_data_lists = [m.split() for m in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train w2v model\n",
    "w2v_model = word2vec.Word2Vec(train_data_lists, size=100, min_count=3, iter=3, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "w2v_model.save('w2v_models/size100_mincount3_iter3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899.75511289\n"
     ]
    }
   ],
   "source": [
    "# Train w2v model of size 200, min_count 5 and 20 iterations\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "w2v_model2 = word2vec.Word2Vec(train_data_lists, size=200, min_count=5, iter=20, workers=4)\n",
    "\n",
    "# Save model\n",
    "w2v_model2.save('w2v_models/size200_mincount5_iter20')\n",
    "stop = timeit.default_timer()\n",
    "print stop - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trained w2v model\n",
    "w2v_model = word2vec.Word2Vec.load('w2v_models/size200_mincount5_iter20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_w2v(document, model, num_features):\n",
    "    '''\n",
    "    Calculate a feature vector for a document by averaging the \n",
    "    word2vec representations of its constituent words\n",
    "    \n",
    "    Args:\n",
    "        document: a message as a str\n",
    "        model: trained w2v model\n",
    "        num_features: length of feature vector\n",
    "        \n",
    "    Returns:\n",
    "        Feature vector that is the average of the word2vec representations of all words in the document\n",
    "    '''\n",
    "    # Get vocab of the model\n",
    "    vocab = set(model.index2word)\n",
    "    vector = np.zeros(num_features)\n",
    "    num_words = 0\n",
    "    # Split document into list of words\n",
    "    words = document.split()\n",
    "    \n",
    "    # If word exists in vocabulary, add its w2v to sum\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            vector = np.add(vector, model[word])\n",
    "            num_words = num_words + 1\n",
    "        \n",
    "    # Average by the number of words\n",
    "    vector = np.divide(vector, num_words)\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def w2v_predict(model, context, responses, size):\n",
    "    '''\n",
    "    Calculates the cosine similarity between the context and each of the possible responses\n",
    "    \n",
    "    Args:\n",
    "        model: a word2vec model\n",
    "        context: a context that we want to find the response for\n",
    "        responses: list of candidate responses containing the actual response\n",
    "        size: size of word2vec vectors for the model\n",
    "        \n",
    "    Returns:\n",
    "        List of response indices sorted in descending order by cosine similarity with context\n",
    "    '''\n",
    "    context_vec = average_w2v(context, model, size)\n",
    "    sims = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # Calculate cosine similarity between the averaged word2vec vector of response and context\n",
    "        response_vec = average_w2v(response, model, size)\n",
    "        sim = cosine_similarity(context_vec.reshape(1, -1), response_vec.reshape(1, -1))[0][0]\n",
    "        sims.append(sim)\n",
    "    \n",
    "    return np.argsort(sims, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.310307\n",
      "Recall @ 2, 10 total choices: 0.445983\n",
      "Recall @ 5, 10 total choices: 0.70333\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate w2v averaging model's performance\n",
    "y = [w2v_predict(w2v_model, test.Context[x], test.iloc[x,1:].values, 200) for x in range(len(test))]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## doc2vec\n",
    "Doc2vec generates vector embeddings for entire documents, so let's try that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taggedMessage = namedtuple('TaggedMessage', 'words tags')\n",
    "documents = []\n",
    "\n",
    "# Preprocess messages\n",
    "for i, message in enumerate(train_data):\n",
    "    # Split into lists of words\n",
    "    words = message.split()\n",
    "    # Add a tag corresponding to the index of the message\n",
    "    tags = [i]\n",
    "    x = taggedMessage(words, tags)\n",
    "    documents.append(taggedMessage(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040.17123508\n"
     ]
    }
   ],
   "source": [
    "# Train doc2vec model\n",
    "start = timeit.default_timer()\n",
    "d2v = doc2vec.Doc2Vec(documents, size=100, workers=4, iter=5)\n",
    "stop = timeit.default_timer()\n",
    "print stop - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v.save(\"d2v_size100_iter5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5021.152601\n"
     ]
    }
   ],
   "source": [
    "# Train doc2vec model with different parameters\n",
    "start = timeit.default_timer()\n",
    "d2v2 = doc2vec.Doc2Vec(documents, size=200, workers=4, iter=20)\n",
    "stop = timeit.default_timer()\n",
    "print stop - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v2.save(\"d2v_size200_iter20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d2v_predict(model, context, responses):\n",
    "    '''\n",
    "    Calculates the cosine similarity between the context and each of the possible responses\n",
    "    \n",
    "    Args:\n",
    "        model: a doc2vec model\n",
    "        context: a context that we want to find the response for\n",
    "        responses: list of candidate responses containing the actual response\n",
    "        \n",
    "    Returns:\n",
    "        List of response indices sorted in descending order by cosine similarity with context\n",
    "    '''\n",
    "    context_vector = model.infer_vector(context.split())\n",
    "    sims = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # Calculate cosine similarity between the doc2vec vectors of response and context\n",
    "        response_vector = model.infer_vector(response.split())\n",
    "        sim = cosine_similarity(context_vector.reshape(1, -1), response_vector.reshape(1, -1))[0][0]\n",
    "        sims.append(sim)\n",
    "    \n",
    "    return np.argsort(sims, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.333245\n",
      "Recall @ 2, 10 total choices: 0.477378\n",
      "Recall @ 5, 10 total choices: 0.737474\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance with doc2vec model with size 200, 20 iterations\n",
    "y = [d2v_predict(d2v2, test.Context[x], test.iloc[x,1:].values) for x in range(test.shape[0])]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best doc2vec model performs slightly worse than TF-IDF. This may have to do with the length discrepancy between contexts and response. It may be interesting to see how doc2vec performs with datasets with similar message lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.280391\n",
      "Recall @ 2, 10 total choices: 0.423044\n",
      "Recall @ 5, 10 total choices: 0.70222\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance with doc2vec model with size 100, 5 iterations\n",
    "y = [d2v_predict(d2v, test.Context[x], test.iloc[x,1:].values) for x in range(test.shape[0])]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the doc2vec model which took less training time performs more poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export best doc2vec model to tsv\n",
    "Next let's export our best doc2vec model's vectors to tsv for visualization with Google's embedding projector (http://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a tsv file with one doc2vec per row\n",
    "with open('tsv/doc2vec_first10000.tsv', 'w') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "#     for v in d2v2.docvecs:\n",
    "#         writer.writerow(v)\n",
    "    # Take a subset for now\n",
    "    for i in range(10000):\n",
    "        writer.writerow(d2v2.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write tsv file with metadata (i.e. all the training text, one per line)\n",
    "with open('tsv/metadata_first10000.tsv', 'w') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for doc in train_data[:10000]:\n",
    "        writer.writerow([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "LDA is a well known topic modelling algorithm from which you can learn the topic distributions of a corpus and infer the topic distribution for a new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate corpus for use with LDA\n",
    "texts = [doc.split() for doc in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary for corpus\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "# Save dictionary for use later\n",
    "dictionary.save('dict/ubuntu.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create corpus of bag of words\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "gensim.corpora.MmCorpus.serialize('/corpora/ubuntu_bow.mm', corpus)  # store to disk, for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "# Train LDA model\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=100, workers=4)\n",
    "stop = timeit.default_timer()\n",
    "print stop - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trained LDA model\n",
    "lda = gensim.models.LdaModel.load(\"LDA/lda_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved dictionary\n",
    "dictionary = gensim.corpora.dictionary.Dictionary.load(\"dict/ubuntu.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lda_feature_vector(topic_dist, num_topics):\n",
    "    '''\n",
    "    Convert gensim LDA topic distribution for a given document to feature vector of length number of topics\n",
    "    \n",
    "    Args:\n",
    "        topic_dist: topic distribution for a message from gensim LDA model\n",
    "                    a list of (topic index, proportion) pairs\n",
    "                    i.e. [(5, 0.59), (12, 0.11)...]\n",
    "        num_topics: total number of topics in the model, for use as length of feature vector\n",
    "            \n",
    "    Returns:\n",
    "        Feature vector for document where the index corresponds to topic number and value is proportion for that topic\n",
    "    '''\n",
    "    vector = np.zeros(num_topics)\n",
    "    \n",
    "    # Fill in values for nonzero topics\n",
    "    for index, value in topic_dist:\n",
    "        vector[index] = value\n",
    "            \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_predict(model, dictionary, context, responses):\n",
    "    '''\n",
    "    Calculates the cosine similarity between the context and each of the possible responses,\n",
    "    returning a ranked list of responses sorted in decreasing order by cosine similarity\n",
    "    \n",
    "    Args:\n",
    "        model: a lda model\n",
    "        dictionary: dictionary associated with the lda model\n",
    "        context: a context that we want to find the response for\n",
    "        responses: list of candidate responses containing the actual response\n",
    "        \n",
    "    Returns:\n",
    "        List of response indices sorted in descending order by cosine similarity with context\n",
    "    '''\n",
    "    # Infer topic distribution and vectorize\n",
    "    context_vector = get_lda_feature_vector(model[dictionary.doc2bow(context.split())], model.num_topics)\n",
    "    sims = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # Calculate cosine similarity between the lda feature vectors of response and context\n",
    "        response_vector = get_lda_feature_vector(model[dictionary.doc2bow(response.split())], model.num_topics)\n",
    "        sim = cosine_similarity(context_vector.reshape(1, -1), response_vector.reshape(1, -1))[0][0]\n",
    "        sims.append(sim)\n",
    "        \n",
    "    return np.argsort(sims, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 1, 10 total choices: 0.309\n",
      "Recall @ 2, 10 total choices: 0.451\n",
      "Recall @ 5, 10 total choices: 0.58\n",
      "Recall @ 10, 10 total choices: 1\n",
      "5038.85787487\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance with lda model\n",
    "y = [lda_predict(lda, dictionary, test.Context[x], test.iloc[x,1:].values) for x in range(len(test.shape[0]))]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-Means Clustering and Naive Bayes\n",
    "Proof of concept for the system that we are implementing. First we cluster the doc2vec vectorizations of each text using k-means. Then we train a naive bayes classifier using the clustering for labels. We generate vectors for new texts using the posterior probabilities for each cluster based on Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load trained doc2vec model\n",
    "d2v2 = doc2vec.Doc2Vec.load(\"d2v_models/d2v_size200_iter20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array of doc2vec vectors\n",
    "X = []\n",
    "for v in d2v2.docvecs:\n",
    "    X.append(v)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster messages using minibatch k-means\n",
    "mbkmeans = MiniBatchKMeans(n_clusters=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nb_predict(nb_model, count_vectorizer, context, responses):\n",
    "    '''\n",
    "    Calculates the cosine similarity between the context and each of the possible responses\n",
    "    \n",
    "    Args:\n",
    "        nb_model: a trained naive bayes classifier\n",
    "        count_vectorizer: a fitted bow vectorizer\n",
    "        context: a context that we want to find the response for\n",
    "        responses: list of candidate responses containing the actual response\n",
    "        \n",
    "    Returns:\n",
    "        List of response indices sorted in descending order by cosine similarity with context\n",
    "    '''\n",
    "    # Get vector of probabilities for each of the clusters for the context\n",
    "    context_vector = nb_model.predict_proba(count_vectorizer.transform([context]).toarray())[0]\n",
    "    sims = []\n",
    "    \n",
    "    for response in responses:\n",
    "        # Calculate cosine similarity between the doc2vec vectors of response and context\n",
    "        response_vector = nb_model.predict_proba(count_vectorizer.transform([response]).toarray())[0]\n",
    "        sim = cosine_similarity(context_vector.reshape(1, -1), response_vector.reshape(1, -1))[0][0]\n",
    "        sims.append(sim)\n",
    "            \n",
    "    return np.argsort(sims, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create bag of words representations for each of the texts\n",
    "count_vect = CountVectorizer()\n",
    "X_bow = count_vect.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0004712b4176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train Multinomial naive bayes classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmultinomial_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmultinomial_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmbkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "# Train Multinomial naive bayes classifier\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_bow.toarray()[:10], mbkmeans.labels_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.51684795e-18   9.24607838e-01   7.53921625e-02]]\n",
      "[[  1.18041346e-13   1.81221104e-01   8.18778896e-01]]\n",
      "[[  3.87616104e-14   3.58390088e-01   6.41609912e-01]]\n",
      "[[  1.75890554e-18   4.29647376e-01   5.70352624e-01]]\n",
      "[[  2.09020531e-10   9.68401824e-01   3.15981753e-02]]\n",
      "[[  1.02014615e-13   9.38612509e-01   6.13874906e-02]]\n",
      "[[  2.41004570e-11   2.39820264e-01   7.60179736e-01]]\n",
      "[[  1.45330987e-15   9.99368452e-01   6.31547625e-04]]\n",
      "[[  5.81412793e-13   9.54916118e-02   9.04508388e-01]]\n",
      "[[  2.20276920e-13   3.13209247e-02   9.68679075e-01]]\n",
      "[[  3.31692298e-21   1.03537809e-01   8.96462191e-01]]\n",
      "[[  2.15349034e-23   9.99963609e-01   3.63908651e-05]]\n",
      "[[  7.15334686e-09   9.79933306e-01   2.00666864e-02]]\n",
      "[[  3.18445373e-22   9.99962239e-01   3.77613430e-05]]\n",
      "[[  5.53036571e-25   9.99490056e-01   5.09944125e-04]]\n",
      "[[  1.73461167e-21   3.08171998e-01   6.91828002e-01]]\n",
      "[[  3.60954009e-22   1.45767420e-01   8.54232580e-01]]\n",
      "[[  1.46896377e-21   1.43330990e-01   8.56669010e-01]]\n",
      "[[  8.48194426e-44   9.99997821e-01   2.17876656e-06]]\n",
      "[[  4.69365759e-14   5.52267575e-01   4.47732425e-01]]\n",
      "Recall @ 1, 10 total choices: 0.15\n",
      "Recall @ 2, 10 total choices: 0.2\n",
      "Recall @ 5, 10 total choices: 0.5\n",
      "Recall @ 10, 10 total choices: 1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance with clustering + multinomial naive bayes approach\n",
    "y = [nb_predict(multinomial_nb, count_vect, test.Context[x], test.iloc[x,1:].values) for x in range(20)]\n",
    "for k in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ {}, 10 total choices: {:g}\".format(k, recall_at_k(y, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
